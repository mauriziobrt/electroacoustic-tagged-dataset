---
layout: post
title:  "Tutorial - Machine Learning for Artists - 1 - Control a Max synthesizer with Machine Learning using Wekinator"
date:   2023-02-01 12:11:54 +0200
categories: mla
---

---

<br>

>  üì° [Click here to download the tutorial patch](https://drive.google.com/file/d/1v3v8dnACwEq126FzKtKkEsVn3XI_9zo9/view?usp=drive_link) and [click here to download the presentation](https://drive.google.com/file/d/1sjuKc-T7cEzvvtbMI9I0KDOgYT-6o7cz/view?usp=drive_link)

> ‚ö†Ô∏è Max Msp is needed to follow this tutorial

<br>

---

<br>

<style>
.video-holder {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
  overflow: hidden;
}
.video-holder iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>
<div class="video-holder">
  <iframe width="560"
          height="315" 
          src="https://drive.google.com/file/d/1q-VjHgYx5I2b4okpBBiw4Wyh8fG3XVrr/preview" 
          frameborder="0" 
          allowfullscreen></iframe>
</div>



<br>

<h1 align="right">What is machine learning </h1>

<br>



<br>

‚ÄúLearning abstract representation of patterns from a collection of examples is precisely what machine learning techniques are tailored to do. The term ‚Äòmachine learning‚Äô refers to a family of algorithms that allow computers to automatically construct abstract representations of problems by learning them from data of some kind. More specifically, artificial neural networks (or neural networks, for short) have been developed to automatically detect patterns in data, and then to use such patterns to classify new data or to predict future data. Rather importantly, while classifying, they often build in their deepest layers a latent (possibly entangled) representation, providing interesting information on some (possibly non-standard) characteristics of the input.‚Äú[^1]



<br>

---



```Machine learning is useful because it allows to speed up a number of processes and, in certain cases, to make them more accurate and efficient.```


---

<br>

<h1>What is machine listening</h1>

<br>



<br>

Machine listening is a class of applied artificial intelligence used to render audio intelligible for machines. In contrast to signal-processing techniques that hear,<mark> machine-listening software is designed to understand sound through the training of an artificial listening mind</mark>. Inspired by the mechanisms underlying human aurality, machine listeners perform intelligent operations on audio through the computational modelling of human perception and cognition. Here, ‚Äúnaturalized‚Äù conceptions of human listening act as templates to endow computer audition with capacities that are meant to simulate our own. [^2]

<br>

---

<br>

<h1>Music Data Representation</h1>

<br>



<br>

Musical data representations can be divided into two main groups: symbolic data (like a midi file or sheet music) and acoustic data (a wav file, a time-series description of the audio).
For this reason machine learning models application to music are usually divided around this dichotomy. 

<br>

---

<br>

<h1>Commercial Use of Machine Learning for Music</h1>

<br>



<br>

| DSP Processes | Music Information Retrieval | Composition Assistant | Synthesis |
|-----|-----|-----|
|Denoising, mixing, mastering, Source Separation | Music Recommendation Systems, Audio Fingerprinting| Symbolic data generation, music production copilot | Audio and music generation |
|Izotope (Ozone, Nectar, RX)|Spotify, Youtube, Shazam, Soundhound|Sony CSL Flow Machines, AIVA| Synthgpt, suno.ai, harmonai |


<br>

---

<br>

<h1>Tutorial - Using Wekinator's models to control a synthesizer in Max MSP</h1>

<br>



<br>

First step first, download Wekinator from [this](http://www.wekinator.org/downloads/) link.

Si tratta di un software sviluppato da Rebecca Fiebrink, ideato per artisti e creativi e che permette di utilizzare velocemente alcune semplici tecniche di machine learning.

‚Ä¢ Utilizza il protocollo OSC per interfacciarsi con altri software, ad esempio Max/MSP, TouchDesigner, Pure Data.


Apriamo Wekinator, apparir√† la schermata ‚ÄúCreate new project‚Äù.

Nella sezione Receiving OSC viene indicata la porta OSC da cui Wekinator ricever√† gli input.

Nella sezione Inputs definiamo il messaggio che deve essere preposto ai dati che vogliamo mandare e il numero di dati. Mettiamo 2 inputs.

Nella sezione Outputs viene indicato il messaggio preposto ai dati e il numero di dati in uscita, in questo caso scriviamo 10. Poi l‚Äôhost che in questo caso rimane localhost (pu√≤ essere utile se utilizziamo pi√π computer in rete) e la porta di uscita. Infine il tipo di modello, per ora vedremo All continuos.

Premiamo Next > in basso a destra.


In alto a sinistra sono indicati i messaggi OSC in e OSC out, i quali diventano verdi quando viene mandato un messaggio.

Appena sotto il bottone start recording permette di registrare i valori che mandiamo in input ad OSC in.

A destra troviamo i due output che abbiamo creato.

Ora occupiamoci della configurazione di OSC in Max.



In Max, per ricevere e mandare i messaggi OSC e comunicare con Wekinator utilizziamo gli oggetti updreceive e udpsend.

Per comunicare con l‚Äôinput di Wekinator utilizziamo udpsend, scriviamo il nome dell‚Äôhost indicato precedentemente e l‚Äôindirizzo. Inoltre con prepend mettiamo davanti al messaggio /wek/inputs in modo che Wekinator capisca il tipo di dato che stiamo mandando.

Per ricevere l‚Äôoutput dobbiamo usare udpreceive e indicare come argomento l‚Äôindirizzo. Come vediamo nel message box, il messaggio ricevuto indicher√† /wek/outputs come primo elemento.

Wekinator di default riceve solo valori compresi tra 0 e 1.


In Max creiamo l‚Äôinterfaccia per il controllo dei preset, usiamo pictslider.

Poi scaliamo i suoi valori con zmap per portarli tra 0 e 1.

Infine creiamo un unico messaggio da mandare a Wekinator nel formato:

/wek/inputs 0. 0.


Come sintetizzatore utilizzeremo il ‚Äúchaotic synthesiser‚Äù, incluso negli allegati.

Con zl.slice rimuoviamo la prima parte del messaggio che cita /wek/outputs e mandiamo i parametri al multislider, che controlla la sintesi.


Ora ci occuperemo del training del modello.

Premiamo randomize, sotto Values, con l‚Äôaudio di Max acceso, per cercare un suono che ci piace.

Una volta trovato premiamo il tasto Start Recording e muoviamo lo slider nella posizione dove vogliamo ritrovare quel suono. Poi premiamo stop recording e ripetiamo questi step.

Una volta ottenuti tutti i dati necessari premiamo il tasto train. Completato il processo di training il led sotto la dicitura status diventer√† verde e potremo utilizzare il modello.

Infine premiamo Run e come potrete sentire avremo ottenuto un mapping immediato dei preset ed una loro interpolazione.

Ci sono diversi tipi di modelli e non tutti danno valori continui, altri infatti permettono di ottenere valori discreti per rappresentare i pattern riconosciuti in input.

---

<br>

<h1>Classification vs Regression</h1>

<br>

![Albuquerque, New Mexico](https://www.simplilearn.com/ice9/free_resources_article_thumb/Regression_vs_Classification.jpg)
*¬©copyright (probablyü´•) Classification vs Regression, from Google search.*

Per classificazione intendiamo l‚Äôutilizzo del machine learning per ottenere dei valori discreti che descrivano il contenuto in ingresso. Per regressione lineare intendiamo l‚Äôutilizzo del machine learning per ottenere dei valori continui, ottenendo una stima dei valori attesi.

---

<br>

<style>
.video-holder {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
  overflow: hidden;
}
.video-holder iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>
<div class="video-holder">
  <iframe width="560"
          height="315" 
          src="https://www.youtube.com/embed/yGBPjv2Sgbk?si=6sXJueedwZ_IcH3V" 
          frameborder="0" 
          allowfullscreen></iframe>
</div>

<br>

Il brano √® ispirato dallo spazio circolare della Roundhouse in cui viene eseguito e dalla poesia di Evgenia Emets. La parte visiva √® costruita come un sistema particellare in un campo di vettori, nello spazio sono presenti 6 microfoni per i cantanti del coro, mappati a 6 reti neurali (chiamate brains) che controllano le caratteristiche di 6 emettitori di particelle, con una ulteriore rete connessa ad un microfono posizionato al centro del cerchio.

L‚Äôinstallazione √® composta da una performance del coro di 2 parti, con un intermezzo nel quale l‚Äôaudience √® incoraggiata ad interagire direttamente con ciascuno dei microfoni.[^3]

<br>

---

<br>

Per approfondire Wekinator e l‚Äôutilizzo di gesture per il controllo musicale consiglio i seguenti link:

[Wekinator Tutorial](https://www.youtube.com/watch?v=dPV-gCqy9j4)

[Kadenze Course on Machine Learning](https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info)

<br>

[^1]: This is the footnote. 
[^2]: [Music Information Retrieval Course](https://musicinformationretrieval.com/)
[^3]: Oliver Gingrich, Evgenia Emets, Alain Renaud, Sean Soraghan, Dario Villanueva Ablanedo; KIMA: The Wheel‚ÄìVoice Turned into Vision: A Participatory, Immersive Visual Soundscape Installation. Leonardo 2020; 53 (5): 479‚Äì484