---
layout: post
title:  "Machine Learning for Artists - 1 - Control a Max synthesizer with Machine Learning using Wekinator"
date:   2023-02-01 12:11:54 +0200
categories: mla
---

---

<br>

>  üì° [Click here to download the tutorial patch](https://drive.google.com/file/d/1v3v8dnACwEq126FzKtKkEsVn3XI_9zo9/view?usp=drive_link) and [click here to download the presentation](https://drive.google.com/file/d/1sjuKc-T7cEzvvtbMI9I0KDOgYT-6o7cz/view?usp=drive_link)

> ‚ö†Ô∏è Max Msp is needed to follow this tutorial

<br>

---

<br>

<style>
.video-holder {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
  overflow: hidden;
}
.video-holder iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>
<div class="video-holder">
  <iframe width="560"
          height="315" 
          src="https://drive.google.com/file/d/1q-VjHgYx5I2b4okpBBiw4Wyh8fG3XVrr/preview" 
          frameborder="0" 
          allowfullscreen></iframe>
</div>



<br>

<h1 align="right">What is machine learning </h1>

<br>



<br>

Creating abstract representations of patterns from a set of examples lies at the heart of what machine learning techniques excel at. In the realm of audio artistry, this capability opens up a world of possibilities. Machine learning, a broad term encompassing a suite of algorithms, empowers computers to autonomously construct abstract representations by learning from data. At the forefront of this revolution are artificial neural networks, commonly referred to as neural networks. These sophisticated systems are designed to discern intricate patterns within data, enabling tasks like classification and prediction. Crucially, as they classify, neural networks delve deep into the data, unraveling latent representations that often reveal unique insights into the input‚Äîpotentially uncovering unconventional characteristics. Join us as we delve into the fascinating intersection of machine learning and audio artistry, exploring how these tools can transform your creative process.



<br>

---



```Machine learning is useful because it allows to speed up a number of processes and, in certain cases, to make them more accurate and efficient.```


---

<br>

<h1>What is machine listening</h1>

<br>



<br>

Machine listening is a class of applied artificial intelligence used to render audio intelligible for machines. In contrast to signal-processing techniques that hear,<mark> machine-listening software is designed to understand sound through the training of an artificial listening mind</mark>. Inspired by the mechanisms underlying human aurality, machine listeners perform intelligent operations on audio through the computational modelling of human perception and cognition. Here, ‚Äúnaturalized‚Äù conceptions of human listening act as templates to endow computer audition with capacities that are meant to simulate our own. [^2]

<br>

---

<br>

<h1>Music Data Representation</h1>

<br>

![](https://miro.medium.com/v2/resize:fit:1400/0*Mn2JcpR-LEov2IXv)
[¬© Towards Data Science - Shunit Haviv Hakimi - BebopNet: Neural Models for Jazz Improvisations](https://towardsdatascience.com/bebopnet-neural-models-for-jazz-improvisations-4a4d723d0b60)

<br>

Musical data representations can be divided into two main groups: symbolic data (like a midi file or sheet music) and acoustic data (a wav file, a time-series description of the audio).
For this reason machine learning models application to music are usually divided around this dichotomy. 

<br>

---

<br>

<h1>Commercial Use of Machine Learning for Music</h1>

<br>



<br>

| DSP Processes | Music Information Retrieval | Composition Assistant | Synthesis |
|-----|-----|-----|
|Denoising, mixing, mastering, Source Separation | Music Recommendation Systems, Audio Fingerprinting| Symbolic data generation, music production copilot | Audio and music generation |
|Izotope (Ozone, Nectar, RX)|Spotify, Youtube, Shazam, Soundhound|Sony CSL Flow Machines, AIVA| Synthgpt, suno.ai, harmonai |


<br>

---

<br>

<h2>Tutorial - Using Wekinator's models to control a synthesizer in Max MSP</h2>

<br>

Wekinator is a software developed by Rebecca Fiebrink, created to provide artists and creatives with an easy to use tool to implement machine learning algorithms into their projects.

The software integrates well with a number of other softwares, because it uses the OSC protocol for communication. For this reason it is easy to connect it with Max msp, Pure Data, TourchDesigner and many other softwares.

In this tutorial, we will learn how to control multiple values of a synthesizer by just using a single control object.

First step: download Wekinator from [this](http://www.wekinator.org/downloads/) link.


<br>

---

<br>

<h4>Create a New Project</h4>

Open Wekinator - you'll be prompted with a "Create a new project" window. Let's analyze what each of the given fields means.

<br>

![](/assets/images/wekinator_first.png)

----

<br>

<h4>Inputs</h4>

In the "Receiving OSC" line we can set the OSC port from where Wekinator will receive its inputs.

Within the OSC message field of the Inputs group, we can define the message that has to be prepended in front of every OSC message in order for it to be accepted by Wekinator. We also have to decide how many Inputs we want to give to the machine - for now, let's put 2 inputs.

<h4>Outputs</h4>

Within the Outputs group we can set the OSC message that is going to be prepended to Wekinator's outputs, then we have to decide how many outputs we want. It is also important to set the Host IP, which in this case we are. going to set to localhost, and also the output door.

We can then set the model type. You are going to see 3 options: all continuous (regression), all classifiers (classification) and dynamic time warping.

<br>

---

<br>

<h1>Classification vs Regression</h1>

<br>

![Albuquerque, New Mexico](https://www.simplilearn.com/ice9/free_resources_article_thumb/Regression_vs_Classification.jpg)
*[¬©copyright Classification vs Regression ](https://www.simplilearn.com/ice9/free_resources_article_thumb/Regression_vs_Classification.jpg)*

---
<br>

By classification we mean using machine learning to obtain discrete values that describe the input values. For example we could classify whether a picture is of a dog or a cat, by describing dog as 0 and cat as 1. The model's scope would to learn how to provide the correct category for each cat/Dog picture that we provide him.

By regression instead we mean the use of machine learning to obtain continuous values. Given a series of points in a cartesian space, the model has to approximate a function that matches all the points in the training space.

<br>

---

<br>

Now that we know the difference between classification and regression, let's press Next in the bottom right of the window.

A new window will appear. In the upper left, we can see two yellow buttons, they will become green if Wekinator receives or sends a message.
Right below them, we can see the start recording button. This button allows us to create new points in the dataset by taking the OSC in data, which will be used to train the model. 
On the right part of the window we'll see the outputs that we have generated.

<br>

---

<br>

Let's move Max Msp. Create a new Max patch and declare two objects: udpreceive and udpsend. These objects allow us to use the OSC protocol within Max. In the udpsend declaration we have to include the host name that we wrote while creating the wekinator project and the OSC input port. Before sending messages to Wekinator we have to prepend "/wek/inputs", else the messages will not be received by the software.
To receive the model's output we need to use udpreceive, with as argument the OSC out port. We will receive a message with "/wek/outputs" as first element.

By default all the data we have to send to Wekinator has to be within the values of 0 and 1.

Let's create now the interface to control our synthesizer, define an object pictslider. His values are between 1 and 127, for this reason we have to map them using zmap to scale them to a range between 0 and 1. Lastly we'll create the single message that we'll send to Wekinator - the format has to be the following: 

	/wek/inputs 0. 0.

As a synthesizer we are going to use the ‚Äúchaotic synthesiser‚Äù(provided in the Flucoma documentation) that you can download at the top of this tutorial.

Now let's configure the Max input part.

We use the object zl.slice to split the received message (/wek/outputs . .) e we'll send the parameters to a multislider, controlling the synthesis.

Now everything is set up and we can start training the model. 

<br>

---

<br>

Let's press randomize, below Values, with Max msp, audio on and let's find a sound we like.
After finding it, we have to press Start Recording in Wekinator, and then move the pictslider in the position we want to fix the sound. Then press Stop Recording. Repeat the steps to map all the positions and sounds you would like to map.

After mapping all the data you wanted, press the train button. After completing the training the led below the status column in Wekinator will become green, meaning that we are ready to use the model.

Now press Run, and move the pictslider. As you can see now you can interpolate between the different presets that you have defined with almost no effort. You just trained your first Neural Network!

<br>

---

<br>

<h4>KIMA - The wheel</h4>

<br>

<style>
.video-holder {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
  overflow: hidden;
}
.video-holder iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>
<div class="video-holder">
  <iframe width="560"
          height="315" 
          src="https://www.youtube.com/embed/yGBPjv2Sgbk?si=6sXJueedwZ_IcH3V" 
          frameborder="0" 
          allowfullscreen></iframe>
</div>

<br>

---

<br>

The composition is inspired by the circular space of the Roundhouse, where it is performed and by Evgenia Emets's poetry. The visual component is built upon a particle system in a vector field. In the performance space there are 6 microphones for the choir's singers. Each one of these microphones is mapped to a neural network which controls the characteristics of the visual component. Moreover there's a 7th microphone, positioned at the center of the space that take care of other visual parameters. The installation is divided into three parts, the first one is a choir performance, then there's an "intermezzo" where the audience is encouraged to interact directly with the microphones. After this interactive part there's a second choir performance.[^3]

<br>

---

<br>

To learn more about Wekinator and how to use gestures to control musical instruments I suggest to read the following links

[Wekinator Tutorial](https://www.youtube.com/watch?v=dPV-gCqy9j4)

[Kadenze Course on Machine Learning](https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info)

<br>

[^1]: This is the footnote. 
[^2]: [Technosphere Magazine](https://technosphere-magazine.hkw.de/p/Machine-Listening-kmgQVZVaQeugBaizQjmZnY)
[^3]: Oliver Gingrich, Evgenia Emets, Alain Renaud, Sean Soraghan, Dario Villanueva Ablanedo; KIMA: The Wheel‚ÄìVoice Turned into Vision: A Participatory, Immersive Visual Soundscape Installation. Leonardo 2020; 53 (5): 479‚Äì484